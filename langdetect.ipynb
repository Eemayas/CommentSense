{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ftlangdetect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\NextJS\\commentsense\\langdetect.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mftlangdetect\u001b[39;00m \u001b[39mimport\u001b[39;00m detect\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m result \u001b[39m=\u001b[39m detect(text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBug√ºn hava √ßok g√ºzel\u001b[39m\u001b[39m\"\u001b[39m, low_memory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ftlangdetect'"
     ]
    }
   ],
   "source": [
    "# from ftlangdetect import detect\n",
    "\n",
    "# result = detect(text=\"Bug√ºn hava √ßok g√ºzel\", low_memory=False)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'language_detection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\NextJS\\commentsense\\langdetect.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Test the code\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHello, world!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m detected_language \u001b[39m=\u001b[39m language_detection(text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDetected language: \u001b[39m\u001b[39m{\u001b[39;00mdetected_language\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'language_detection' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the code\n",
    "text = \"Hello, world!\"\n",
    "detected_language = language_detection(text)\n",
    "print(f\"Detected language: {detected_language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en with confidence: 0.9999977366622846\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, detect_langs\n",
    "\n",
    "def detect_language_with_confidence(text, confidence_threshold=0.5):\n",
    "    try:\n",
    "        # Detect the language of the text\n",
    "        language = detect(text)\n",
    "        \n",
    "        # Get the confidence score\n",
    "        confidence = detect_langs(text)[0].prob\n",
    "        \n",
    "        # Check if the confidence is above the threshold\n",
    "        if confidence >= confidence_threshold:\n",
    "            return language, confidence\n",
    "        else:\n",
    "            return \"Undetermined\", confidence\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle exceptions, e.g., if the text is too short\n",
    "        return \"Error\", 0.0\n",
    "\n",
    "# Example usage\n",
    "text_to_detect = \"Hello, this is an example text.\"\n",
    "language, confidence = detect_language_with_confidence(text_to_detect)\n",
    "\n",
    "print(f\"Detected language: {language} with confidence: {confidence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2297517534.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    >>> {'label': 'positive'}\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tweetnlp\n",
    "\n",
    "# ENGLISH MODEL\n",
    "model = tweetnlp.load_model('sentiment')  # Or `model = tweetnlp.Sentiment()` \n",
    "model.sentiment(\"Yes, including Medicare and social security savingüëç\")  # Or `model.predict`\n",
    "model.sentiment(\"Yes, including Medicare and social security savingüëç\", return_probability=True)\n",
    "\n",
    "model = tweetnlp.load_model('sentiment', multilingual=True)  # Or `model = tweetnlp.Sentiment(multilingual=True)` \n",
    "model.sentiment(\"Â§©Ê∞ó„ÅåËâØ„ÅÑ„Å®„ÇÑ„Å£„Å±„ÇäÊ∞óÊåÅ„Å°ËâØ„ÅÑ„Å™„ÅÇ‚ú®\")\n",
    "model.sentiment(\"Â§©Ê∞ó„ÅåËâØ„ÅÑ„Å®„ÇÑ„Å£„Å±„ÇäÊ∞óÊåÅ„Å°ËâØ„ÅÑ„Å™„ÅÇ‚ú®\", return_probability=True)\n",
    "\n",
    "# GET DATASET (ENGLISH)\n",
    "dataset, label2id = tweetnlp.load_dataset('sentiment')\n",
    "for l in ['all', 'arabic', 'english', 'french', 'german', 'hindi', 'italian', 'portuguese', 'spanish']:\n",
    "    dataset_multilingual, label2id_multilingual = tweetnlp.load_dataset('sentiment', multilingual=True, task_language=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00340221 0.02262291 0.9739749 ]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "\n",
    "\n",
    "# TF\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL,local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL,local_files_only=True)\n",
    "# tokenizer.save_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "text = \"i love you üòä\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "scores = output[0][0].numpy()\n",
    "\n",
    "scores = softmax(scores)\n",
    "print(scores)\n",
    "# ranking = np.argsort(scores)\n",
    "# print(scores)\n",
    "# ranking = ranking[::-1]\n",
    "# for i in range(scores.shape[0]):\n",
    "#     l = labels[ranking[i]]\n",
    "#     s = scores[ranking[i]]\n",
    "#     print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "\n",
    "lstm = load_model('app/api/flask/sentimentmodel.h5')\n",
    "rnn = load_model('app/api/flask/85.59%RNN_Model.h5')\n",
    "gru = load_model('app/api/flask/GRU_sentimentmodel.h5')\n",
    "\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "with open('app/api/flask/tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "# with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "    # tokenizer_RNN= pickle.load(tokenizer_file)\n",
    "tokenizer_RNN=Tokenizer()\n",
    "with open('app/api/flask/RNNtokenizer.pkl', 'rb') as tokenizer_file_RNN:\n",
    "# with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "    tokenizer_RNN= pickle.load(tokenizer_file_RNN)\n",
    "    # tokenizer_RNN= pickle.load(tokenizer_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove emoji\t This song helped me through one of the absolute hardest times of my life having seizures and loosing my driving license for a year and a half.horrible,horrible time,but I had THIS\n",
      "filter english comment=====\t This song helped me through one of the absolute hardest times of my life having seizures and loosing my driving license for a year and a half.horrible,horrible time,but I had THIS\n",
      "preproceesf\t .song help one absolut hardest time life seizur loos drive licens year half horribl horribl time\n",
      "song help one absolut hardest time life seizur loos drive licens year half horribl horribl time\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "{'comment': 'This song helped me through one of the absolute hardest times of my life having seizures and loosing my driving license for a year and a half.horrible,horrible time,but I had THIS‚ù§‚ù§‚ù§‚ù§', 'type': 2, 'negative_score': 2.54, 'neutral_score': 95.21, 'positive_score': 2.24}\n",
      "remove emoji\t Deep meaning, Clear pronounce.... Nice song....\n",
      "filter english comment=====\t Deep meaning, Clear pronounce. Nice song\n",
      "preproceesf\t .deep mean clear pronounc nice song\n",
      "deep mean clear pronounc nice song\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "{'comment': 'Deep meaning, Clear pronounce....‚ù§ Nice song....', 'type': 2, 'negative_score': 2.58, 'neutral_score': 95.51, 'positive_score': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import re\n",
    "from langdetect import detect,detect_langs\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def removeemoji(text):\n",
    "    # Define a regular expression pattern to match emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # Alphabetic Presentation Forms\n",
    "                               \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               \"\\U0001FB00-\\U0001FBFF\"  # Symbols for Legacy Computing\n",
    "                               \"\\U0001FC00-\\U0001FCFF\"  # Symbols for Legacy Computing\n",
    "                               \"\\U0001FD00-\\U0001FDFF\"  # Symbols for Legacy Computing\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # Alphabetic Presentation Forms\n",
    "                               \"\\U0001FE00-\\U0001FEFF\"  # Variation Selectors\n",
    "                               \"\\U0001FF00-\\U0001FFFF\"  # Variation Selectors\n",
    "                               \"\\U0001F200-\\U0001F251\"\n",
    "                               \"‚ù§\"\n",
    "                               \"‚ù§Ô∏è\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    # Use the sub method to remove emojis\n",
    "    text_no_emojis = emoji_pattern.sub(r'', text)\n",
    "    print(f\"remove emoji\\t {text_no_emojis}\")\n",
    "    return text_no_emojis\n",
    "\n",
    "def filter_english_comments(text):\n",
    "  # we ue re module for multi seperator split\n",
    "    sentences=re.split(r'[.:]',text)\n",
    "    englishcomments=[]\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            # print(sentence)\n",
    "            # print(detect(sentence))\n",
    "            if detect(sentence)==\"en\" and detect_langs(sentence)[0].prob>=0.7:\n",
    "                englishcomments.append(sentence)\n",
    "            else:\n",
    "                englishcomments.append(\"\")\n",
    "        except:\n",
    "            pass\n",
    "    filteredcomment='.'.join(englishcomments)\n",
    "    print(f\"filter english comment=====\\t {filteredcomment}\")\n",
    "    return filteredcomment\n",
    "\n",
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    '''Removes HTML tags: replaces anything between opening and closing <> with empty space'''\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "\n",
    "contractions = {\n",
    "    \"ilove\":\"i love\",\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "      \"aint\": \"am not\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"couldve\": \"could have\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"hed\": \"he would\",\n",
    "    \"hell\": \"he will\",\n",
    "    \"hes\": \"he is\",\n",
    "    \"howd\": \"how did\",\n",
    "    \"howll\": \"how will\",\n",
    "    \"hows\": \"how is\",\n",
    "    \"Id\": \"I would\",\n",
    "    \"Ill\": \"I will\",\n",
    "    \"Im\": \"I am\",\n",
    "    \"Ive\": \"I have\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"itd\": \"it would\",\n",
    "    \"itll\": \"it will\",\n",
    "    \"its\": \"it is\",\n",
    "    \"lets\": \"let us\",\n",
    "    \"mightve\": \"might have\",\n",
    "    \"mightnt\": \"might not\",\n",
    "    \"mustve\": \"must have\",\n",
    "    \"mustnt\": \"must not\",\n",
    "    \"neednt\": \"need not\",\n",
    "    \"oughtnt\": \"ought not\",\n",
    "    \"shant\": \"shall not\",\n",
    "    \"shed\": \"she would\",\n",
    "    \"shell\": \"she will\",\n",
    "    \"shes\": \"she is\",\n",
    "    \"shouldve\": \"should have\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"thered\": \"there would\",\n",
    "    \"therell\": \"there will\",\n",
    "    \"theres\": \"there is\",\n",
    "    \"theyd\": \"they would\",\n",
    "    \"theyll\": \"they will\",\n",
    "    \"theyre\": \"they are\",\n",
    "    \"theyve\": \"they have\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"wed\": \"we would\",\n",
    "    \"well\": \"we will\",\n",
    "    \"were\": \"we are\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"werent\": \"were not\",\n",
    "    \"whatll\": \"what will\",\n",
    "    \"whatre\": \"what are\",\n",
    "    \"whats\": \"what is\",\n",
    "    \"whatve\": \"what have\",\n",
    "    \"whered\": \"where did\",\n",
    "    \"wheres\": \"where is\",\n",
    "    \"whod\": \"who would\",\n",
    "    \"wholl\": \"who will\",\n",
    "    \"whos\": \"who is\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldve\": \"would have\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"youd\": \"you would\",\n",
    "    \"youll\": \"you will\",\n",
    "    \"youre\": \"you are\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"hed\": \"he would\",\n",
    "    \"hell\": \"he will\",\n",
    "    \"hes\": \"he is\",\n",
    "    \"howd\": \"how did\",\n",
    "    \"howll\": \"how will\",\n",
    "    \"hows\": \"how is\",\n",
    "    \"Id\": \"I would\",\n",
    "    \"Ill\": \"I will\",\n",
    "    \"Im\": \"I am\",\n",
    "    \"Ive\": \"I have\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"itd\": \"it would\",\n",
    "    \"itll\": \"it will\",\n",
    "    \"its\": \"it is\",\n",
    "    \"lets\": \"let us\",\n",
    "    \"mightve\": \"might have\",\n",
    "    \"mightnt\": \"might not\",\n",
    "    \"mustve\": \"must have\",\n",
    "    \"mustnt\": \"must not\",\n",
    "    \"neednt\": \"need not\",\n",
    "    \"oughtnt\": \"ought not\",\n",
    "    \"shant\": \"shall not\",\n",
    "    \"shed\": \"she would\",\n",
    "    \"shell\": \"she will\",\n",
    "    \"shes\": \"she is\",\n",
    "    \"shouldve\": \"should have\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"thered\": \"there would\",\n",
    "    \"therell\": \"there will\",\n",
    "    \"theres\": \"there is\",\n",
    "    \"theyd\": \"they would\",\n",
    "    \"theyll\": \"they will\",\n",
    "    \"theyre\": \"they are\",\n",
    "    \"theyve\": \"they have\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"wed\": \"we would\",\n",
    "    \"well\": \"we will\",\n",
    "    \"were\": \"we are\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"werent\": \"were not\",\n",
    "    \"whatll\": \"what will\",\n",
    "    \"whatre\": \"what are\",\n",
    "    \"whats\": \"what is\",\n",
    "    \"whatve\": \"what have\",\n",
    "    \"whered\": \"where did\",\n",
    "    \"wheres\": \"where is\",\n",
    "    \"whod\": \"who would\",\n",
    "    \"wholl\": \"who will\",\n",
    "    \"whos\": \"who is\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldve\": \"would have\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"youd\": \"you would\",\n",
    "    \"youll\": \"you will\",\n",
    "    \"youre\": \"you are\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"Idve\": \"I would have\",\n",
    "    \"hedve\": \"he would have\",\n",
    "    \"itdve\": \"it would have\",\n",
    "    \"shedve\": \"she would have\",\n",
    "    \"wedve\": \"we would have\",\n",
    "    \"theydve\": \"they would have\",\n",
    "    \"shouldve\": \"should have\",\n",
    "    \"shouldntve\": \"should not have\",\n",
    "    \"couldve\": \"could have\",\n",
    "    \"couldntve\": \"could not have\",\n",
    "    \"mightve\": \"might have\",\n",
    "    \"mightntve\": \"might not have\",\n",
    "    \"mustve\": \"must have\",\n",
    "    \"mustntve\": \"must not have\",\n",
    "    \"needntve\": \"need not have\",\n",
    "    \"oughtntve\": \"ought not have\",\n",
    "    \"shantve\": \"shall not have\",\n",
    "    \"wontve\": \"will not have\",\n",
    "    \"wouldve\": \"would have\",\n",
    "    \"wouldntve\": \"would not have\",\n",
    "    \"youdve\": \"you would have\",\n",
    "    \"youllve\": \"you will have\",\n",
    "    \"youre\": \"you are\",\n",
    "    \"youve\": \"you have\"\n",
    "}\n",
    "\n",
    "def preprocessing_RNN(text):\n",
    "    text=str(text)\n",
    "    # Convert to lowercase\n",
    "    text=text.lower()\n",
    "\n",
    "    # Remove html tags\n",
    "    text= remove_tags(text)\n",
    "\n",
    "    # Substitute 'n't' with 'not'\n",
    "    text = re.sub(r\"n't\", \"not\",text)\n",
    "    \n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ',text)\n",
    "\n",
    "    # Single character removal\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)  # When we remove apostrophe from the word \"Mark's\", the apostrophe is replaced by an empty space. Hence, we are left with single character \"s\" that we are removing here.\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Next, we remove all the single characters and replace it by a space which creates multiple spaces in our text. Finally, we remove the multiple spaces from our text as well.\n",
    "\n",
    "    # Remove Stopwords\n",
    "    words_to_remove = ['y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "                    'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
    "                    'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
    "                    'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n",
    "                    'won', \"won't\", 'wouldn', \"wouldn't\",\"not\"]\n",
    "    # Get the default English stopwords from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Remove the specified words from the default stopwords\n",
    "    stop_words = [word for word in stop_words if word not in words_to_remove]\n",
    "   \n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
    "    text = pattern.sub('', text)\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    tokens = [contractions[word] if word in contractions else word for word in tokens]\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == 'not' and i < len(tokens) - 1:\n",
    "            tokens[i + 1] = 'not_' + tokens[i + 1]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    processed_text = ' '.join(tokens)\n",
    "    print(f\"preproceesf\\t .{processed_text}\")\n",
    "    return processed_text\n",
    "\n",
    "def clean_RNN(text):\n",
    "    sent=removeemoji(text)\n",
    "    # print(sent)\n",
    "    sent=filter_english_comments(sent)\n",
    "    # print(sent)\n",
    "    sent=preprocessing_RNN(sent)\n",
    "    print(sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "comments=[\"This song helped me through one of the absolute hardest times of my life having seizures and loosing my driving license for a year and a half.horrible,horrible time,but I had THIS‚ù§‚ù§‚ù§‚ù§\",\"Deep meaning, Clear pronounce....‚ù§ Nice song....\"]\n",
    "for comment in comments:\n",
    "    initComment=comment\n",
    "    comment=clean_RNN(comment)\n",
    "    if comment!=\" \" and comment!=\".\":\n",
    "        sequence=tokenizer.texts_to_sequences([comment])\n",
    "        padded_sequences = pad_sequences(sequence,maxlen=50)\n",
    "        prediction=lstm.predict(padded_sequences)\n",
    "        prediction=prediction.tolist()\n",
    "        result=prediction[0]\n",
    "        type=np.argmax(np.array(result))\n",
    "        type=0 if type==0 else 4 if type==2 else 2\n",
    "        new_row = {'comment': initComment, \"type\":type,'negative_score': round(result[0]*100, 2), 'neutral_score': round(result[1]*100, 2), 'positive_score': round(result[2]*100, 2)}\n",
    "        print(new_row)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
