{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ftlangdetect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\NextJS\\commentsense\\langdetect.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mftlangdetect\u001b[39;00m \u001b[39mimport\u001b[39;00m detect\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m result \u001b[39m=\u001b[39m detect(text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBug√ºn hava √ßok g√ºzel\u001b[39m\u001b[39m\"\u001b[39m, low_memory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ftlangdetect'"
     ]
    }
   ],
   "source": [
    "from ftlangdetect import detect\n",
    "\n",
    "result = detect(text=\"Bug√ºn hava √ßok g√ºzel\", low_memory=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'language_detection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\NextJS\\commentsense\\langdetect.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Test the code\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHello, world!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m detected_language \u001b[39m=\u001b[39m language_detection(text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDetected language: \u001b[39m\u001b[39m{\u001b[39;00mdetected_language\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'language_detection' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the code\n",
    "text = \"Hello, world!\"\n",
    "detected_language = language_detection(text)\n",
    "print(f\"Detected language: {detected_language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en with confidence: 0.9999977366622846\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, detect_langs\n",
    "\n",
    "def detect_language_with_confidence(text, confidence_threshold=0.5):\n",
    "    try:\n",
    "        # Detect the language of the text\n",
    "        language = detect(text)\n",
    "        \n",
    "        # Get the confidence score\n",
    "        confidence = detect_langs(text)[0].prob\n",
    "        \n",
    "        # Check if the confidence is above the threshold\n",
    "        if confidence >= confidence_threshold:\n",
    "            return language, confidence\n",
    "        else:\n",
    "            return \"Undetermined\", confidence\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle exceptions, e.g., if the text is too short\n",
    "        return \"Error\", 0.0\n",
    "\n",
    "# Example usage\n",
    "text_to_detect = \"Hello, this is an example text.\"\n",
    "language, confidence = detect_language_with_confidence(text_to_detect)\n",
    "\n",
    "print(f\"Detected language: {language} with confidence: {confidence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 929/929 [00:00<00:00, 932kB/s]\n",
      "Downloading tf_model.h5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 499M/499M [00:18<00:00, 27.2MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading vocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 899k/899k [00:00<00:00, 1.06MB/s]\n",
      "Downloading merges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456k/456k [00:00<00:00, 2.07MB/s]\n",
      "Downloading (‚Ä¶)cial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 239/239 [00:00<00:00, 217kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.3490569591522217}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\", tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "sentiment_task(\"Covid cases are increasing fast!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2297517534.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    >>> {'label': 'positive'}\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tweetnlp\n",
    "\n",
    "# ENGLISH MODEL\n",
    "model = tweetnlp.load_model('sentiment')  # Or `model = tweetnlp.Sentiment()` \n",
    "model.sentiment(\"Yes, including Medicare and social security savingüëç\")  # Or `model.predict`\n",
    ">>> {'label': 'positive'}\n",
    "model.sentiment(\"Yes, including Medicare and social security savingüëç\", return_probability=True)\n",
    ">>> {'label': 'positive', 'probability': {'negative': 0.004584966693073511, 'neutral': 0.19360853731632233, 'positive': 0.8018065094947815}}\n",
    "\n",
    "# MULTILINGUAL MODEL\n",
    "model = tweetnlp.load_model('sentiment', multilingual=True)  # Or `model = tweetnlp.Sentiment(multilingual=True)` \n",
    "model.sentiment(\"Â§©Ê∞ó„ÅåËâØ„ÅÑ„Å®„ÇÑ„Å£„Å±„ÇäÊ∞óÊåÅ„Å°ËâØ„ÅÑ„Å™„ÅÇ‚ú®\")\n",
    ">>> {'label': 'positive'}\n",
    "model.sentiment(\"Â§©Ê∞ó„ÅåËâØ„ÅÑ„Å®„ÇÑ„Å£„Å±„ÇäÊ∞óÊåÅ„Å°ËâØ„ÅÑ„Å™„ÅÇ‚ú®\", return_probability=True)\n",
    ">>> {'label': 'positive', 'probability': {'negative': 0.028369612991809845, 'neutral': 0.08128828555345535, 'positive': 0.8903420567512512}}\n",
    "\n",
    "# GET DATASET (ENGLISH)\n",
    "dataset, label2id = tweetnlp.load_dataset('sentiment')\n",
    "# GET DATASET (MULTILINGUAL)\n",
    "for l in ['all', 'arabic', 'english', 'french', 'german', 'hindi', 'italian', 'portuguese', 'spanish']:\n",
    "    dataset_multilingual, label2id_multilingual = tweetnlp.load_dataset('sentiment', multilingual=True, task_language=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00340221 0.02262291 0.9739749 ]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "\n",
    "\n",
    "# TF\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL,local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL,local_files_only=True)\n",
    "# tokenizer.save_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "text = \"i love you üòä\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "scores = output[0][0].numpy()\n",
    "\n",
    "scores = softmax(scores)\n",
    "print(scores)\n",
    "# ranking = np.argsort(scores)\n",
    "# print(scores)\n",
    "# ranking = ranking[::-1]\n",
    "# for i in range(scores.shape[0]):\n",
    "#     l = labels[ranking[i]]\n",
    "#     s = scores[ranking[i]]\n",
    "#     print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
