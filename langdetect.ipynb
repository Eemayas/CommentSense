{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ftlangdetect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\NextJS\\commentsense\\langdetect.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mftlangdetect\u001b[39;00m \u001b[39mimport\u001b[39;00m detect\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m result \u001b[39m=\u001b[39m detect(text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBug√ºn hava √ßok g√ºzel\u001b[39m\u001b[39m\"\u001b[39m, low_memory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ftlangdetect'"
     ]
    }
   ],
   "source": [
    "# from ftlangdetect import detect\n",
    "\n",
    "# result = detect(text=\"Bug√ºn hava √ßok g√ºzel\", low_memory=False)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'language_detection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\NextJS\\commentsense\\langdetect.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Test the code\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHello, world!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m detected_language \u001b[39m=\u001b[39m language_detection(text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDetected language: \u001b[39m\u001b[39m{\u001b[39;00mdetected_language\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'language_detection' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the code\n",
    "text = \"Hello, world!\"\n",
    "detected_language = language_detection(text)\n",
    "print(f\"Detected language: {detected_language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en with confidence: 0.9999977366622846\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, detect_langs\n",
    "\n",
    "def detect_language_with_confidence(text, confidence_threshold=0.5):\n",
    "    try:\n",
    "        # Detect the language of the text\n",
    "        language = detect(text)\n",
    "        \n",
    "        # Get the confidence score\n",
    "        confidence = detect_langs(text)[0].prob\n",
    "        \n",
    "        # Check if the confidence is above the threshold\n",
    "        if confidence >= confidence_threshold:\n",
    "            return language, confidence\n",
    "        else:\n",
    "            return \"Undetermined\", confidence\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle exceptions, e.g., if the text is too short\n",
    "        return \"Error\", 0.0\n",
    "\n",
    "# Example usage\n",
    "text_to_detect = \"Hello, this is an example text.\"\n",
    "language, confidence = detect_language_with_confidence(text_to_detect)\n",
    "\n",
    "print(f\"Detected language: {language} with confidence: {confidence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2297517534.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    >>> {'label': 'positive'}\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tweetnlp\n",
    "\n",
    "# ENGLISH MODEL\n",
    "model = tweetnlp.load_model('sentiment')  # Or `model = tweetnlp.Sentiment()` \n",
    "model.sentiment(\"Yes, including Medicare and social security savingüëç\")  # Or `model.predict`\n",
    "model.sentiment(\"Yes, including Medicare and social security savingüëç\", return_probability=True)\n",
    "\n",
    "model = tweetnlp.load_model('sentiment', multilingual=True)  # Or `model = tweetnlp.Sentiment(multilingual=True)` \n",
    "model.sentiment(\"Â§©Ê∞ó„ÅåËâØ„ÅÑ„Å®„ÇÑ„Å£„Å±„ÇäÊ∞óÊåÅ„Å°ËâØ„ÅÑ„Å™„ÅÇ‚ú®\")\n",
    "model.sentiment(\"Â§©Ê∞ó„ÅåËâØ„ÅÑ„Å®„ÇÑ„Å£„Å±„ÇäÊ∞óÊåÅ„Å°ËâØ„ÅÑ„Å™„ÅÇ‚ú®\", return_probability=True)\n",
    "\n",
    "# GET DATASET (ENGLISH)\n",
    "dataset, label2id = tweetnlp.load_dataset('sentiment')\n",
    "for l in ['all', 'arabic', 'english', 'french', 'german', 'hindi', 'italian', 'portuguese', 'spanish']:\n",
    "    dataset_multilingual, label2id_multilingual = tweetnlp.load_dataset('sentiment', multilingual=True, task_language=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00340221 0.02262291 0.9739749 ]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Tasks:\n",
    "# emoji, emotion, hate, irony, offensive, sentiment\n",
    "# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "\n",
    "\n",
    "# TF\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL,local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL,local_files_only=True)\n",
    "# tokenizer.save_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "text = \"i love you üòä\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "scores = output[0][0].numpy()\n",
    "\n",
    "scores = softmax(scores)\n",
    "print(scores)\n",
    "# ranking = np.argsort(scores)\n",
    "# print(scores)\n",
    "# ranking = ranking[::-1]\n",
    "# for i in range(scores.shape[0]):\n",
    "#     l = labels[ranking[i]]\n",
    "#     s = scores[ranking[i]]\n",
    "#     print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "\n",
    "lstm = load_model('app/api/flask/sentimentmodel.h5')\n",
    "rnn = load_model('app/api/flask/85.59%RNN_Model.h5')\n",
    "gru = load_model('app/api/flask/GRU_sentimentmodel.h5')\n",
    "\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "with open('app/api/flask/tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "# with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "    # tokenizer_RNN= pickle.load(tokenizer_file)\n",
    "tokenizer_RNN=Tokenizer()\n",
    "with open('app/api/flask/RNNtokenizer.pkl', 'rb') as tokenizer_file_RNN:\n",
    "# with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "    tokenizer_RNN= pickle.load(tokenizer_file_RNN)\n",
    "    # tokenizer_RNN= pickle.load(tokenizer_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove emoji\t This song helped me through one of the absolute hardest times of my life having seizures and loosing my driving license for a year and a half.horrible,horrible time,but I had THIS\n",
      "filter english comment=====\t This song helped me through one of the absolute hardest times of my life having seizures and loosing my driving license for a year and a half.horrible,horrible time,but I had THIS\n",
      "preproceesf\t .song help one absolut hardest time life seizur loos drive licens year half horribl horribl time\n",
      "song help one absolut hardest time life seizur loos drive licens year half horribl horribl time\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_1/embedding_1/embedding_lookup defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\prash\\AppData\\Local\\Temp\\ipykernel_17964\\2847008349.py\", line 451, in <module>\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2655, in predict\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 590, in __call__\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\sequential.py\", line 398, in call\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n\nindices[0,92] = 15486 is not in [0, 10000)\n\t [[{{node sequential_1/embedding_1/embedding_lookup}}]] [Op:__inference_predict_function_5117]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32me:\\NextJS\\commentsense\\langdetect.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#X14sZmlsZQ%3D%3D?line=448'>449</a>\u001b[0m sequence\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39mtexts_to_sequences([comment])\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#X14sZmlsZQ%3D%3D?line=449'>450</a>\u001b[0m padded_sequences \u001b[39m=\u001b[39m pad_sequences(sequence,maxlen\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#X14sZmlsZQ%3D%3D?line=450'>451</a>\u001b[0m prediction\u001b[39m=\u001b[39mrnn\u001b[39m.\u001b[39;49mpredict(padded_sequences)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#X14sZmlsZQ%3D%3D?line=451'>452</a>\u001b[0m prediction\u001b[39m=\u001b[39mprediction\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/NextJS/commentsense/langdetect.ipynb#X14sZmlsZQ%3D%3D?line=452'>453</a>\u001b[0m result\u001b[39m=\u001b[39mprediction[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sequential_1/embedding_1/embedding_lookup defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n\n  File \"C:\\Users\\prash\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n\n  File \"C:\\Users\\prash\\AppData\\Local\\Temp\\ipykernel_17964\\2847008349.py\", line 451, in <module>\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2655, in predict\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 590, in __call__\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\sequential.py\", line 398, in call\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n\n  File \"c:\\Users\\prash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n\nindices[0,92] = 15486 is not in [0, 10000)\n\t [[{{node sequential_1/embedding_1/embedding_lookup}}]] [Op:__inference_predict_function_5117]"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import re\n",
    "from langdetect import detect,detect_langs\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def removeemoji(text):\n",
    "    # Define a regular expression pattern to match emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # Alphabetic Presentation Forms\n",
    "                               \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               \"\\U0001FB00-\\U0001FBFF\"  # Symbols for Legacy Computing\n",
    "                               \"\\U0001FC00-\\U0001FCFF\"  # Symbols for Legacy Computing\n",
    "                               \"\\U0001FD00-\\U0001FDFF\"  # Symbols for Legacy Computing\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # Alphabetic Presentation Forms\n",
    "                               \"\\U0001FE00-\\U0001FEFF\"  # Variation Selectors\n",
    "                               \"\\U0001FF00-\\U0001FFFF\"  # Variation Selectors\n",
    "                               \"\\U0001F200-\\U0001F251\"\n",
    "                               \"‚ù§\"\n",
    "                               \"‚ù§Ô∏è\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    # Use the sub method to remove emojis\n",
    "    text_no_emojis = emoji_pattern.sub(r'', text)\n",
    "    print(f\"remove emoji\\t {text_no_emojis}\")\n",
    "    return text_no_emojis\n",
    "\n",
    "def filter_english_comments(text):\n",
    "  # we ue re module for multi seperator split\n",
    "    sentences=re.split(r'[.:]',text)\n",
    "    englishcomments=[]\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            # print(sentence)\n",
    "            # print(detect(sentence))\n",
    "            if detect(sentence)==\"en\" and detect_langs(sentence)[0].prob>=0.7:\n",
    "                englishcomments.append(sentence)\n",
    "            else:\n",
    "                englishcomments.append(\"\")\n",
    "        except:\n",
    "            pass\n",
    "    filteredcomment='.'.join(englishcomments)\n",
    "    print(f\"filter english comment=====\\t {filteredcomment}\")\n",
    "    return filteredcomment\n",
    "\n",
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    '''Removes HTML tags: replaces anything between opening and closing <> with empty space'''\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "\n",
    "contractions = {\n",
    "    \"ilove\":\"i love\",\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "      \"aint\": \"am not\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"couldve\": \"could have\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"hed\": \"he would\",\n",
    "    \"hell\": \"he will\",\n",
    "    \"hes\": \"he is\",\n",
    "    \"howd\": \"how did\",\n",
    "    \"howll\": \"how will\",\n",
    "    \"hows\": \"how is\",\n",
    "    \"Id\": \"I would\",\n",
    "    \"Ill\": \"I will\",\n",
    "    \"Im\": \"I am\",\n",
    "    \"Ive\": \"I have\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"itd\": \"it would\",\n",
    "    \"itll\": \"it will\",\n",
    "    \"its\": \"it is\",\n",
    "    \"lets\": \"let us\",\n",
    "    \"mightve\": \"might have\",\n",
    "    \"mightnt\": \"might not\",\n",
    "    \"mustve\": \"must have\",\n",
    "    \"mustnt\": \"must not\",\n",
    "    \"neednt\": \"need not\",\n",
    "    \"oughtnt\": \"ought not\",\n",
    "    \"shant\": \"shall not\",\n",
    "    \"shed\": \"she would\",\n",
    "    \"shell\": \"she will\",\n",
    "    \"shes\": \"she is\",\n",
    "    \"shouldve\": \"should have\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"thered\": \"there would\",\n",
    "    \"therell\": \"there will\",\n",
    "    \"theres\": \"there is\",\n",
    "    \"theyd\": \"they would\",\n",
    "    \"theyll\": \"they will\",\n",
    "    \"theyre\": \"they are\",\n",
    "    \"theyve\": \"they have\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"wed\": \"we would\",\n",
    "    \"well\": \"we will\",\n",
    "    \"were\": \"we are\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"werent\": \"were not\",\n",
    "    \"whatll\": \"what will\",\n",
    "    \"whatre\": \"what are\",\n",
    "    \"whats\": \"what is\",\n",
    "    \"whatve\": \"what have\",\n",
    "    \"whered\": \"where did\",\n",
    "    \"wheres\": \"where is\",\n",
    "    \"whod\": \"who would\",\n",
    "    \"wholl\": \"who will\",\n",
    "    \"whos\": \"who is\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldve\": \"would have\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"youd\": \"you would\",\n",
    "    \"youll\": \"you will\",\n",
    "    \"youre\": \"you are\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"hed\": \"he would\",\n",
    "    \"hell\": \"he will\",\n",
    "    \"hes\": \"he is\",\n",
    "    \"howd\": \"how did\",\n",
    "    \"howll\": \"how will\",\n",
    "    \"hows\": \"how is\",\n",
    "    \"Id\": \"I would\",\n",
    "    \"Ill\": \"I will\",\n",
    "    \"Im\": \"I am\",\n",
    "    \"Ive\": \"I have\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"itd\": \"it would\",\n",
    "    \"itll\": \"it will\",\n",
    "    \"its\": \"it is\",\n",
    "    \"lets\": \"let us\",\n",
    "    \"mightve\": \"might have\",\n",
    "    \"mightnt\": \"might not\",\n",
    "    \"mustve\": \"must have\",\n",
    "    \"mustnt\": \"must not\",\n",
    "    \"neednt\": \"need not\",\n",
    "    \"oughtnt\": \"ought not\",\n",
    "    \"shant\": \"shall not\",\n",
    "    \"shed\": \"she would\",\n",
    "    \"shell\": \"she will\",\n",
    "    \"shes\": \"she is\",\n",
    "    \"shouldve\": \"should have\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"thered\": \"there would\",\n",
    "    \"therell\": \"there will\",\n",
    "    \"theres\": \"there is\",\n",
    "    \"theyd\": \"they would\",\n",
    "    \"theyll\": \"they will\",\n",
    "    \"theyre\": \"they are\",\n",
    "    \"theyve\": \"they have\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"wed\": \"we would\",\n",
    "    \"well\": \"we will\",\n",
    "    \"were\": \"we are\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"werent\": \"were not\",\n",
    "    \"whatll\": \"what will\",\n",
    "    \"whatre\": \"what are\",\n",
    "    \"whats\": \"what is\",\n",
    "    \"whatve\": \"what have\",\n",
    "    \"whered\": \"where did\",\n",
    "    \"wheres\": \"where is\",\n",
    "    \"whod\": \"who would\",\n",
    "    \"wholl\": \"who will\",\n",
    "    \"whos\": \"who is\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldve\": \"would have\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"youd\": \"you would\",\n",
    "    \"youll\": \"you will\",\n",
    "    \"youre\": \"you are\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"Idve\": \"I would have\",\n",
    "    \"hedve\": \"he would have\",\n",
    "    \"itdve\": \"it would have\",\n",
    "    \"shedve\": \"she would have\",\n",
    "    \"wedve\": \"we would have\",\n",
    "    \"theydve\": \"they would have\",\n",
    "    \"shouldve\": \"should have\",\n",
    "    \"shouldntve\": \"should not have\",\n",
    "    \"couldve\": \"could have\",\n",
    "    \"couldntve\": \"could not have\",\n",
    "    \"mightve\": \"might have\",\n",
    "    \"mightntve\": \"might not have\",\n",
    "    \"mustve\": \"must have\",\n",
    "    \"mustntve\": \"must not have\",\n",
    "    \"needntve\": \"need not have\",\n",
    "    \"oughtntve\": \"ought not have\",\n",
    "    \"shantve\": \"shall not have\",\n",
    "    \"wontve\": \"will not have\",\n",
    "    \"wouldve\": \"would have\",\n",
    "    \"wouldntve\": \"would not have\",\n",
    "    \"youdve\": \"you would have\",\n",
    "    \"youllve\": \"you will have\",\n",
    "    \"youre\": \"you are\",\n",
    "    \"youve\": \"you have\"\n",
    "}\n",
    "\n",
    "def preprocessing_RNN(text):\n",
    "    text=str(text)\n",
    "    # Convert to lowercase\n",
    "    text=text.lower()\n",
    "\n",
    "    # Remove html tags\n",
    "    text= remove_tags(text)\n",
    "\n",
    "    # Substitute 'n't' with 'not'\n",
    "    text = re.sub(r\"n't\", \"not\",text)\n",
    "    \n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ',text)\n",
    "\n",
    "    # Single character removal\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)  # When we remove apostrophe from the word \"Mark's\", the apostrophe is replaced by an empty space. Hence, we are left with single character \"s\" that we are removing here.\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Next, we remove all the single characters and replace it by a space which creates multiple spaces in our text. Finally, we remove the multiple spaces from our text as well.\n",
    "\n",
    "    # Remove Stopwords\n",
    "    words_to_remove = ['y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "                    'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
    "                    'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
    "                    'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n",
    "                    'won', \"won't\", 'wouldn', \"wouldn't\",\"not\"]\n",
    "    # Get the default English stopwords from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Remove the specified words from the default stopwords\n",
    "    stop_words = [word for word in stop_words if word not in words_to_remove]\n",
    "   \n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
    "    text = pattern.sub('', text)\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    tokens = [contractions[word] if word in contractions else word for word in tokens]\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == 'not' and i < len(tokens) - 1:\n",
    "            tokens[i + 1] = 'not_' + tokens[i + 1]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    processed_text = ' '.join(tokens)\n",
    "    print(f\"preproceesf\\t .{processed_text}\")\n",
    "    return processed_text\n",
    "\n",
    "def clean_RNN(text):\n",
    "    sent=removeemoji(text)\n",
    "    # print(sent)\n",
    "    sent=filter_english_comments(sent)\n",
    "    # print(sent)\n",
    "    sent=preprocessing_RNN(sent)\n",
    "    print(sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "comments=[\"This song helped me through one of the absolute hardest times of my life having seizures and loosing my driving license for a year and a half.horrible,horrible time,but I had THIS‚ù§‚ù§‚ù§‚ù§\",\"Deep meaning, Clear pronounce....‚ù§ Nice song....\"]\n",
    "for comment in comments:\n",
    "    initComment=comment\n",
    "    comment=clean_RNN(comment)\n",
    "    if comment!=\" \" and comment!=\".\":\n",
    "        sequence=tokenizer.texts_to_sequences([comment])\n",
    "        padded_sequences = pad_sequences(sequence,maxlen=100)\n",
    "        prediction=rnn.predict(padded_sequences)\n",
    "        prediction=prediction.tolist()\n",
    "        result=prediction[0]\n",
    "        type=np.argmax(np.array(result))\n",
    "        type=0 if type==0 else 4 if type==2 else 2\n",
    "        new_row = {'comment': initComment, \"type\":type,'negative_score': round(result[0]*100, 2), 'neutral_score': round(result[1]*100, 2), 'positive_score': round(result[2]*100, 2)}\n",
    "        print(new_row)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import re\n",
    "from langdetect import detect,detect_langs\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def removeemoji(text):\n",
    "    # Define a regular expression pattern to match emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # Alphabetic Presentation Forms\n",
    "                               \"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               \"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               \"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               \"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               \"\\U0001FB00-\\U0001FBFF\"  # Symbols for Legacy Computing\n",
    "                               \"\\U0001FC00-\\U0001FCFF\"  # Symbols for Legacy Computing\n",
    "                               \"\\U0001FD00-\\U0001FDFF\"  # Symbols for Legacy Computing\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # Alphabetic Presentation Forms\n",
    "                               \"\\U0001FE00-\\U0001FEFF\"  # Variation Selectors\n",
    "                               \"\\U0001FF00-\\U0001FFFF\"  # Variation Selectors\n",
    "                               \"\\U0001F200-\\U0001F251\"\n",
    "                               \"‚ù§\"\n",
    "                               \"‚ù§Ô∏è\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    # Use the sub method to remove emojis\n",
    "    text_no_emojis = emoji_pattern.sub(r'', text)\n",
    "    # print(f\"remove emoji\\t {text_no_emojis}\")\n",
    "    return text_no_emojis\n",
    "\n",
    "def filter_english_comments(text):\n",
    "  # we ue re module for multi seperator split\n",
    "    sentences=re.split(r'[.:]',text)\n",
    "    englishcomments=[]\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            # print(sentence)\n",
    "            # print(detect(sentence))\n",
    "            if detect(sentence)==\"en\" and detect_langs(sentence)[0].prob>=0.7:\n",
    "                englishcomments.append(sentence)\n",
    "            else:\n",
    "                englishcomments.append(\"\")\n",
    "        except:\n",
    "            pass\n",
    "    filteredcomment='.'.join(englishcomments)\n",
    "    # print(f\"filter english comment=====\\t {filteredcomment}\")\n",
    "    return filteredcomment\n",
    "\n",
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    '''Removes HTML tags: replaces anything between opening and closing <> with empty space'''\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "\n",
    "contractions = {\n",
    "    \"ilove\":\"i love\",\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'll\": \"there will\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "      \"aint\": \"am not\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"couldve\": \"could have\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"hed\": \"he would\",\n",
    "    \"hell\": \"he will\",\n",
    "    \"hes\": \"he is\",\n",
    "    \"howd\": \"how did\",\n",
    "    \"howll\": \"how will\",\n",
    "    \"hows\": \"how is\",\n",
    "    \"Id\": \"I would\",\n",
    "    \"Ill\": \"I will\",\n",
    "    \"Im\": \"I am\",\n",
    "    \"Ive\": \"I have\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"itd\": \"it would\",\n",
    "    \"itll\": \"it will\",\n",
    "    \"its\": \"it is\",\n",
    "    \"lets\": \"let us\",\n",
    "    \"mightve\": \"might have\",\n",
    "    \"mightnt\": \"might not\",\n",
    "    \"mustve\": \"must have\",\n",
    "    \"mustnt\": \"must not\",\n",
    "    \"neednt\": \"need not\",\n",
    "    \"oughtnt\": \"ought not\",\n",
    "    \"shant\": \"shall not\",\n",
    "    \"shed\": \"she would\",\n",
    "    \"shell\": \"she will\",\n",
    "    \"shes\": \"she is\",\n",
    "    \"shouldve\": \"should have\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"thered\": \"there would\",\n",
    "    \"therell\": \"there will\",\n",
    "    \"theres\": \"there is\",\n",
    "    \"theyd\": \"they would\",\n",
    "    \"theyll\": \"they will\",\n",
    "    \"theyre\": \"they are\",\n",
    "    \"theyve\": \"they have\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"wed\": \"we would\",\n",
    "    \"well\": \"we will\",\n",
    "    \"were\": \"we are\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"werent\": \"were not\",\n",
    "    \"whatll\": \"what will\",\n",
    "    \"whatre\": \"what are\",\n",
    "    \"whats\": \"what is\",\n",
    "    \"whatve\": \"what have\",\n",
    "    \"whered\": \"where did\",\n",
    "    \"wheres\": \"where is\",\n",
    "    \"whod\": \"who would\",\n",
    "    \"wholl\": \"who will\",\n",
    "    \"whos\": \"who is\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldve\": \"would have\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"youd\": \"you would\",\n",
    "    \"youll\": \"you will\",\n",
    "    \"youre\": \"you are\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"hed\": \"he would\",\n",
    "    \"hell\": \"he will\",\n",
    "    \"hes\": \"he is\",\n",
    "    \"howd\": \"how did\",\n",
    "    \"howll\": \"how will\",\n",
    "    \"hows\": \"how is\",\n",
    "    \"Id\": \"I would\",\n",
    "    \"Ill\": \"I will\",\n",
    "    \"Im\": \"I am\",\n",
    "    \"Ive\": \"I have\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"itd\": \"it would\",\n",
    "    \"itll\": \"it will\",\n",
    "    \"its\": \"it is\",\n",
    "    \"lets\": \"let us\",\n",
    "    \"mightve\": \"might have\",\n",
    "    \"mightnt\": \"might not\",\n",
    "    \"mustve\": \"must have\",\n",
    "    \"mustnt\": \"must not\",\n",
    "    \"neednt\": \"need not\",\n",
    "    \"oughtnt\": \"ought not\",\n",
    "    \"shant\": \"shall not\",\n",
    "    \"shed\": \"she would\",\n",
    "    \"shell\": \"she will\",\n",
    "    \"shes\": \"she is\",\n",
    "    \"shouldve\": \"should have\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"thered\": \"there would\",\n",
    "    \"therell\": \"there will\",\n",
    "    \"theres\": \"there is\",\n",
    "    \"theyd\": \"they would\",\n",
    "    \"theyll\": \"they will\",\n",
    "    \"theyre\": \"they are\",\n",
    "    \"theyve\": \"they have\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"wed\": \"we would\",\n",
    "    \"well\": \"we will\",\n",
    "    \"were\": \"we are\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"werent\": \"were not\",\n",
    "    \"whatll\": \"what will\",\n",
    "    \"whatre\": \"what are\",\n",
    "    \"whats\": \"what is\",\n",
    "    \"whatve\": \"what have\",\n",
    "    \"whered\": \"where did\",\n",
    "    \"wheres\": \"where is\",\n",
    "    \"whod\": \"who would\",\n",
    "    \"wholl\": \"who will\",\n",
    "    \"whos\": \"who is\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldve\": \"would have\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"youd\": \"you would\",\n",
    "    \"youll\": \"you will\",\n",
    "    \"youre\": \"you are\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"Idve\": \"I would have\",\n",
    "    \"hedve\": \"he would have\",\n",
    "    \"itdve\": \"it would have\",\n",
    "    \"shedve\": \"she would have\",\n",
    "    \"wedve\": \"we would have\",\n",
    "    \"theydve\": \"they would have\",\n",
    "    \"shouldve\": \"should have\",\n",
    "    \"shouldntve\": \"should not have\",\n",
    "    \"couldve\": \"could have\",\n",
    "    \"couldntve\": \"could not have\",\n",
    "    \"mightve\": \"might have\",\n",
    "    \"mightntve\": \"might not have\",\n",
    "    \"mustve\": \"must have\",\n",
    "    \"mustntve\": \"must not have\",\n",
    "    \"needntve\": \"need not have\",\n",
    "    \"oughtntve\": \"ought not have\",\n",
    "    \"shantve\": \"shall not have\",\n",
    "    \"wontve\": \"will not have\",\n",
    "    \"wouldve\": \"would have\",\n",
    "    \"wouldntve\": \"would not have\",\n",
    "    \"youdve\": \"you would have\",\n",
    "    \"youllve\": \"you will have\",\n",
    "    \"youre\": \"you are\",\n",
    "    \"youve\": \"you have\"\n",
    "}\n",
    "\n",
    "def preprocessing_RNN(text):\n",
    "    text=str(text)\n",
    "    # Convert to lowercase\n",
    "    text=text.lower()\n",
    "\n",
    "    # Remove html tags\n",
    "    text= remove_tags(text)\n",
    "\n",
    "    # Substitute 'n't' with 'not'\n",
    "    text = re.sub(r\"n't\", \"not\",text)\n",
    "    \n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub('[^a-zA-Z]', ' ',text)\n",
    "\n",
    "    # Single character removal\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)  # When we remove apostrophe from the word \"Mark's\", the apostrophe is replaced by an empty space. Hence, we are left with single character \"s\" that we are removing here.\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Next, we remove all the single characters and replace it by a space which creates multiple spaces in our text. Finally, we remove the multiple spaces from our text as well.\n",
    "\n",
    "    # Remove Stopwords\n",
    "    words_to_remove = ['y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "                    'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
    "                    'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",\n",
    "                    'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n",
    "                    'won', \"won't\", 'wouldn', \"wouldn't\",\"not\"]\n",
    "    # Get the default English stopwords from NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Remove the specified words from the default stopwords\n",
    "    stop_words = [word for word in stop_words if word not in words_to_remove]\n",
    "   \n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
    "    text = pattern.sub('', text)\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    tokens = [contractions[word] if word in contractions else word for word in tokens]\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] == 'not' and i < len(tokens) - 1:\n",
    "            tokens[i + 1] = 'not_' + tokens[i + 1]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    processed_text = ' '.join(tokens)\n",
    "    # print(f\"preproceesf\\t .{processed_text}\")\n",
    "    return processed_text\n",
    "\n",
    "def clean_RNN(text):\n",
    "    sent=removeemoji(text)\n",
    "    # print(sent)\n",
    "    sent=filter_english_comments(sent)\n",
    "    # print(sent)\n",
    "    sent=preprocessing_RNN(sent)\n",
    "    # print(sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "comment=\"This song helped me through one of the absolute hardest times of my life having seizures and loosing my driving license for a year and a half.horrible,horrible time,but I had THIS‚ù§‚ù§‚ù§‚ù§\"\n",
    "initComment=comment\n",
    "comment=clean_RNN(comment)\n",
    "if comment and  comment!=\" \" and comment!=\".\":\n",
    "    sequence=tokenizer.texts_to_sequences([comment])\n",
    "    # padded_sequences = pad_sequences(sequence,maxlen=50)\n",
    "    \n",
    "    # prediction_LSTM=lstm.predict(padded_sequences)\n",
    "    # prediction_LSTM=prediction_LSTM.tolist()\n",
    "    # result_LSTM=prediction[0]\n",
    "    \n",
    "    # prediction_GRU=gru.predict(padded_sequences)\n",
    "    # prediction_GRU=prediction_GRU.tolist()\n",
    "    # result_GRU=prediction[0]\n",
    "\n",
    "    # prediction_Roberta=lstm.predict(padded_sequences)\n",
    "    # prediction_Roberta=prediction_Roberta.tolist()\n",
    "    # result_Roberta=prediction[0]\n",
    "    \n",
    "    padded_sequences = pad_sequences(sequence,maxlen=100)\n",
    "    prediction_RNN=rnn.predict(padded_sequences)\n",
    "    prediction_RNN=prediction_RNN.tolist()\n",
    "    result_RNN=prediction[0]\n",
    "    \n",
    "    # type=np.argmax(np.array(result_Roberta))\n",
    "    # type=0 if type==0 else 4 if type==2 else 2\n",
    "    new_row = {'comment': initComment, \"type\":type,\n",
    "            #    \"LSTM\":{'negative_score': round(result_LSTM[0]*100, 2), 'neutral_score': round(result_LSTM[1]*100, 2), 'positive_score': round(result_LSTM[2]*100, 2)},\n",
    "            #    \"GRU\":{'negative_score': round(result_GRU[0]*100, 2), 'neutral_score': round(result_GRU[1]*100, 2), 'positive_score': round(result_GRU[2]*100, 2)},\n",
    "               \"RNN\":{'negative_score': round(result_RNN[0]*100, 2), 'neutral_score': round(result_RNN[1]*100, 2), 'positive_score': round(result_RNN[2]*100, 2)},\n",
    "            #    \"Roberta\":{'negative_score': round(result_Roberta[0]*100, 2), 'neutral_score': round(result_Roberta[1]*100, 2), 'positive_score': round(result_Roberta[2]*100, 2)}\n",
    "            }\n",
    "    \n",
    "    print(new_row)        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
